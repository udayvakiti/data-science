{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb+jkKyfJTYn7FMcJ0s32B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udayvakiti/data-science/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsHwCqJXpGf-",
        "outputId": "e4963382-2f73-4c87-ae29-4787441b8d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  sentiment            id                            date       query  \\\n",
            "0       \"0\"  \"1467810672\"  \"Mon Apr 06 22:19:49 PDT 2009\"  \"NO_QUERY\"   \n",
            "1       \"0\"  \"1467810917\"  \"Mon Apr 06 22:19:53 PDT 2009\"  \"NO_QUERY\"   \n",
            "2       \"0\"  \"1467811184\"  \"Mon Apr 06 22:19:57 PDT 2009\"  \"NO_QUERY\"   \n",
            "3       \"0\"  \"1467811372\"  \"Mon Apr 06 22:20:00 PDT 2009\"  \"NO_QUERY\"   \n",
            "4       \"0\"  \"1467811592\"  \"Mon Apr 06 22:20:03 PDT 2009\"  \"NO_QUERY\"   \n",
            "\n",
            "              user                                              tweet  \n",
            "0  \"scotthamilton\"  \"is upset that he can't update his Facebook by...  \n",
            "1       \"mattycus\"  \"@Kenichan I dived many times for the ball. Ma...  \n",
            "2        \"ElleCTF\"  \"my whole body feels itchy and like its on fire \"  \n",
            "3       \"joy_wolf\"                    \"@Kwesidei not the whole crew \"  \n",
            "4        \"mybirch\"                                      \"Need a hug \"  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create an empty DataFrame to store the data\n",
        "df = pd.DataFrame(columns=['sentiment', 'id', 'date', 'query', 'user', 'tweet'])\n",
        "\n",
        "# Open the CSV file\n",
        "with open(\"/content/training.1600000.processed.noemoticon.csv\", 'r', encoding='latin-1') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "    # Exclude the problematic rows\n",
        "    for i, line in enumerate(lines):\n",
        "        if i != 35326:  # Skip row 35326\n",
        "            data = line.strip().split(\",\")\n",
        "            if len(data) == 6:  # Ensure the line has the expected number of columns\n",
        "                df.loc[len(df)] = data\n",
        "\n",
        "# Display the loaded data\n",
        "print(df.head())\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'tweet']\n",
        "\n",
        "# Preprocess the tweets\n",
        "df['preprocessed_tweet'] = df['tweet'].apply(preprocess_text)\n",
        "\n",
        "# Create a list of preprocessed tweets\n",
        "preprocessed_tweets = df['preprocessed_tweet'].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the parameters\n",
        "vocab_size = 10000  # Adjust based on your dataset size\n",
        "embedding_dim = 100  # Adjust based on your choice\n",
        "max_length = 20  # Adjust based on the desired length of generated tweets\n",
        "num_epochs = 10  # Adjust based on your choice\n",
        "batch_size = 32  # Adjust based on your choice\n",
        "\n",
        "# Prepare the data\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(preprocessed_tweets)\n",
        "sequences = tokenizer.texts_to_sequences(preprocessed_tweets)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Prepare the training data\n",
        "input_sequences = padded_sequences[:, :-1]\n",
        "labels = tf.keras.utils.to_categorical(padded_sequences[:, -1], num_classes=vocab_size)\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length-1),\n",
        "    tf.keras.layers.LSTM(units=64),\n",
        "    tf.keras.layers.Dense(units=vocab_size, activation='softmax')\n",
        "])\n",
        "test_tweets=[]\n",
        "# Compile and train the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(input_sequences, labels, epochs=num_epochs, batch_size=batch_size)\n",
        "for i in range(10,20):\n",
        "# Generate new tweets\n",
        "    seed_text = \"\"\n",
        "    next_words = i\n",
        "    for _ in range(next_words):\n",
        "    # Tokenize the seed text\n",
        "        seed_sequence = tokenizer.texts_to_sequences([seed_text])\n",
        "        padded_seed = pad_sequences(seed_sequence, maxlen=max_length-1)\n",
        "\n",
        "    # Predict the next word\n",
        "        predicted_probs = model.predict(padded_seed)[0]\n",
        "        predicted_index = tf.random.categorical(predicted_probs.reshape(1, -1), num_samples=1)[-1, 0].numpy()\n",
        "        predicted_word = tokenizer.index_word[predicted_index]\n",
        "\n",
        "    # Append the predicted word to the seed text\n",
        "        seed_text += \" \" + predicted_word\n",
        "\n",
        "    print(\"Generated Tweet:\", seed_text)\n",
        "    test_tweets.append(seed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCi8RYidw0h2",
        "outputId": "f684490b-b958-4357-d38c-338a09071efb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "336/336 [==============================] - 20s 54ms/step - loss: 7.7825 - accuracy: 0.0226\n",
            "Epoch 2/10\n",
            "336/336 [==============================] - 16s 47ms/step - loss: 7.0315 - accuracy: 0.0244\n",
            "Epoch 3/10\n",
            "336/336 [==============================] - 15s 46ms/step - loss: 6.8988 - accuracy: 0.0237\n",
            "Epoch 4/10\n",
            "336/336 [==============================] - 16s 46ms/step - loss: 6.7875 - accuracy: 0.0229\n",
            "Epoch 5/10\n",
            "336/336 [==============================] - 17s 52ms/step - loss: 6.6841 - accuracy: 0.0240\n",
            "Epoch 6/10\n",
            "336/336 [==============================] - 16s 48ms/step - loss: 6.5621 - accuracy: 0.0246\n",
            "Epoch 7/10\n",
            "336/336 [==============================] - 16s 47ms/step - loss: 6.4226 - accuracy: 0.0277\n",
            "Epoch 8/10\n",
            "336/336 [==============================] - 16s 47ms/step - loss: 6.2697 - accuracy: 0.0319\n",
            "Epoch 9/10\n",
            "336/336 [==============================] - 17s 50ms/step - loss: 6.1071 - accuracy: 0.0361\n",
            "Epoch 10/10\n",
            "336/336 [==============================] - 16s 48ms/step - loss: 5.9417 - accuracy: 0.0415\n",
            "1/1 [==============================] - 0s 443ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Generated Tweet:  slipandstumbl footwear loooooooooooooooooooooooooooooooonnnnnnngggggggg woken ballot msia absolut hairstyl champ itslacey\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "Generated Tweet:  doe eighth stare half japannali victoria disqu chishinau reatla via bremen\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Generated Tweet:  swing github vuhhnessa worthi rooibo house suprem roth yet wath mouth jadilah\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Generated Tweet:  feckin arsehol vitamin rj cazm reluctantli nc yaykimo juic rellyab meas paulaabdul unforn\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Generated Tweet:  therapi korean akismet text daughter croup teddi motion billohbil homesick inspiredwrit pugmark leakag pmt\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Generated Tweet:  safe helppp kid area research helenthornb hot alfr price sya frozen stick yaknow snapcas tomatosalsa\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Generated Tweet:  mistak jlsoffici throat marcusneto damag levi sianllewellyn commun falalala yumyum idea everyon rekidk send ten propos\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Generated Tweet:  dismantl sundberg ballpark agoni emiliexclarkex jltorrent naughtyhaughti nooooo roselyn ikea ati chrome myrtti calendar doglet earbud bff\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Generated Tweet:  pooooo pump odd flipmcneil devoir lauri zackdft final string pepper ruddi mamasvan buttt seattl colleg fish chauv edg\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Generated Tweet:  knock gentlemen jennchambless bubbl treatment remedi lc kernel que psa pink umm lsat russbak jordan munci yazeez driven dat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to calculate perplexity\n",
        "def calculate_perplexity(model, test_data):\n",
        "    # Predict the probabilities of the test data\n",
        "    predictions = model.predict(test_data)\n",
        "    \n",
        "    # Calculate perplexity\n",
        "    cross_entropy = np.log(predictions[np.arange(len(predictions)), test_data[:, -1]]) # Get the cross-entropy for the correct words\n",
        "    perplexity = np.exp(-np.mean(cross_entropy))\n",
        "    \n",
        "    return perplexity\n",
        "\n",
        "# Preprocess the test data\n",
        "test_sequences = tokenizer.texts_to_sequences(test_tweets)\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_length)\n",
        "\n",
        "# Calculate perplexity\n",
        "perplexity = calculate_perplexity(model, test_sequences)\n",
        "print('Perplexity:', perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4SYFyaG3AJN",
        "outputId": "29fa199e-3e9b-4b38-a2be-b55543a009db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 465ms/step\n",
            "Perplexity: 731565.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Generate tweets\n",
        "reference_tweets = [test_tweets[0],test_tweets[1],test_tweets[2], test_tweets[3], test_tweets[4],test_tweets[5],test_tweets[6],test_tweets[7],test_tweets[8],test_tweets[9]]\n",
        "\n",
        "# Calculate BLEU scores\n",
        "bleu_scores = []\n",
        "for reference, generated in zip(reference_tweets, test_tweets):\n",
        "    reference_tokens = [reference.split()]\n",
        "    generated_tokens = generated.split()\n",
        "    bleu_score = sentence_bleu(reference_tokens, generated_tokens)\n",
        "    bleu_scores.append(bleu_score)\n",
        "print(bleu_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKVk2Our3EAF",
        "outputId": "f89a1500-b86c-499b-cecc-20e893e3d46f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "from rouge import Rouge\n",
        "\n",
        "# Generate tweets\n",
        "reference_tweets = [test_tweets[0],test_tweets[1],test_tweets[2], test_tweets[3], test_tweets[4],test_tweets[5],test_tweets[6],test_tweets[7],test_tweets[8],test_tweets[9]]\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "rouge = Rouge()\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores = rouge.get_scores(test_tweets, reference_tweets, avg=True)\n",
        "\n",
        "# Access the average ROUGE score\n",
        "average_rouge_score = scores['rouge-l']['f']\n",
        "print(average_rouge_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBwfVga34vYP",
        "outputId": "9b2d4fc0-9ee9-49c1-c5e1-36672b541ec8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "0.9999999949999998\n"
          ]
        }
      ]
    }
  ]
}